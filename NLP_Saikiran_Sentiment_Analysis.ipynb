{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdjew1TtjNZj"
   },
   "source": [
    "## Assignmnet No.2\n",
    "### NLP - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9gTuAtsjNZm"
   },
   "source": [
    "Submitted by : **Saikiran N. Pasikanti**<br>\n",
    "compiled online at colab.research.google.com with GPU kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H37yzT0sO4UC"
   },
   "source": [
    "# Sentiment Analysis on NLTK - Twitter Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAKcJ3HrjNbZ"
   },
   "source": [
    "**Task2** :The goal here is to predict the sentiments behind tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter :** People tweet about their life, events and express opinion about various topics using text messages limited to 140 characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ve0EbUirjNbZ"
   },
   "source": [
    "# (1) Using \"twitter_samples\" from nltk\n",
    " - NLTK's Twitter corpus currently contains a sample of 10k Tweets (named 'twitter_samples') which are divided according to sentiment into negative and positive\n",
    " - Tweets are stored in the form of .json\n",
    " - negative_tweets.json = 5k, positive_tweets.json = 5k, tweets.20150430-223406.json = 20k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1523491588833,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "Y6LuJCBTu1Ir",
    "outputId": "265e347d-3e1e-4628-ccfe-c35351d53c70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CorpusReader.abspath of <TwitterCorpusReader in 'C:\\\\Users\\\\Sai\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\twitter_samples'>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                                     ## import Natural Language Tool Kit\n",
    "#nltk.download(\"twitter_samples\")               ## download and unzip all the text files of twitter samples\n",
    "from nltk.corpus import twitter_samples as ts   ## import twitter_samples text files as ts\n",
    "ts.abspath                                      ## location of files where they are stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fNDModsQLmo"
   },
   "source": [
    "Sample tweets from nltk twitter_samples corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4808,
     "status": "ok",
     "timestamp": 1523491595918,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "QlYR-4HcWV96",
    "outputId": "06e8f8f2-ba9e-4b8e-92d9-ca71558070e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS =  #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "POS =  @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "NEG =  hopeless for tmr :(\n",
      "NEG =  Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\n",
      "RT @KirkKus: Indirect cost of the UK being in the EU is estimated to be costing Britain Â£170 billion per year! #BetterOffOut #UKIP\n"
     ]
    }
   ],
   "source": [
    "import json                                            ## import json package\n",
    "\n",
    "pos_tweets = ts.strings('positive_tweets.json')        ## 5k tweets with positive sentiment\n",
    "\n",
    "neg_tweets = ts.strings('negative_tweets.json')        ## 5k tweets with negative sentiment\n",
    " \n",
    "all_tweets = ts.strings('tweets.20150430-223406.json') ## 20k positive and negative tweets\n",
    "\n",
    "print(\"POS = \", pos_tweets[0])           \n",
    "print(\"POS = \", pos_tweets[1])          \n",
    "print(\"NEG = \", neg_tweets[0])           \n",
    "print(\"NEG = \", neg_tweets[1])           \n",
    "print(all_tweets[0])              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kM0SRuZfZKai"
   },
   "source": [
    "### Creating a categorized text corpus :\n",
    "For training, we need to first create a list of labeled feature sets.\n",
    "\n",
    "We will convert the Categorized Plaintext Corpus Reader data into python list.\n",
    "\n",
    "This list should be of the form [(featureset, label)], where the featureset variable is a dict and label is the known class label for the featureset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2379,
     "status": "ok",
     "timestamp": 1523491601762,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "pUe6aJjGfnzF",
    "outputId": "7fc45ca7-579f-45b1-ef66-8e73f93929ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)', 'pos')\n",
      "('hopeless for tmr :(', 'neg')\n"
     ]
    }
   ],
   "source": [
    "df = ([(t, \"pos\") for t in pos_tweets] + \n",
    "             [(t, \"neg\") for t in neg_tweets])   ## categorize the text corpus, add labels to text\n",
    "print(df[0])                                                             ## sample tweet with postitive sentiment\n",
    "print(df[5000])                                                          ## sample tweet with negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1523445946961,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "41b4r2PajNcB",
    "outputId": "6dd70914-a37d-4123-dacb-55eaf470b8f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykAMrY3cgxl3"
   },
   "source": [
    "### Splitting the data\n",
    "In our dataset first 5000 tweets have positive sentiment and later have negative sentiment. So we will shuffle the data using shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)                          ## for reproducing same results each time\n",
    "\n",
    "\n",
    "from random import shuffle                ## import required package/ functions\n",
    "shuffle(df)                               ## shuffle the ordered reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aApdhZgThcjS"
   },
   "source": [
    "Now we divide the dataset into train and test.\n",
    "\n",
    "Train (75%) data will be processed in order to use it for training a classification model\n",
    "Test (25%) data will be used for predicting the sentiment out of the review by the model built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1523491610608,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "MvTIFQ9KjNcG",
    "outputId": "729fe4f3-0109-4062-c0dc-4693c9861f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500 : 2500\n"
     ]
    }
   ],
   "source": [
    "ttrain=df[:7500]                           ## 75% of the total data\n",
    "ttest=df[7500:]                            ## remaining 25% of the total data\n",
    "print(len(ttrain),\":\",len(ttest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc2rdqIhh6tE"
   },
   "source": [
    "# (2) Processing the data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_GjcE0SYXrLn"
   },
   "source": [
    "### Labels\n",
    "**Extract the sentiment labels by making positive reviews as class 1 and negative reviews as class 2**<br>\n",
    "This function will take the data in the form of [features, labels] and extracts labels as separate list with levels 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJVCUTkZHR_2"
   },
   "source": [
    "\n",
    "### Extract the sentiment labels by making positive reviews as class 1 and negative reviews as class -1<br>\n",
    "This function will take the data in the form of [features, labels] and extracts labels as separate list with levels 1 and -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "75yXJngLEmTq"
   },
   "outputs": [],
   "source": [
    "def get_lables(data):\n",
    "    labels = []\n",
    "    for tup in data:\n",
    "        if tup[1].lower()==\"neg\":\n",
    "            labels.append(-1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TiLi8xxv6Ur"
   },
   "source": [
    "\n",
    "These categories are exclusive, which makes a classifier trained on them a binary classifier.<br> \n",
    "Binary classifiers have only two classification labels, and will always choose one or the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TLHeY3KoXrLn"
   },
   "outputs": [],
   "source": [
    "ttraining_labels = get_lables(ttrain)\n",
    "\n",
    "ttest_gold_labels = get_lables(ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language used Twitter is used via a variety of media including SMS and mobile phone apps. Because of this and the 140-character limit, language used in Tweets tend be more colloquial, and filled with slang and misspellings. Use of hashtags also gained popularity on Twitter and is a primary feature in any given tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample tweet:**\n",
    "RT@soacecabard TOday's the Day. ICS <3\n",
    "#andriod#google#samsung\n",
    "\n",
    "**attributes:**<br>\n",
    "**RT** = retweet symbol<br>\n",
    "**@spacecanard** = handle<br>\n",
    "**Today's the Dat.** = \".\" punctuation<br>\n",
    "**<3** = emoticon<br>\n",
    "**#andriod** = hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User-generated content on the web is seldom present in a form usable for learning. \n",
    "- It becomes important to normalize the text by applying a series of pre-processing steps. \n",
    "- Following pre-processing steps will be performed to decrease the size of the feature set to make it suitable for learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML decoding\n",
    "It looks like HTML encoding has not been converted to text, and ended up in text field as â&ampâ,â&quotâ,etc. Decoding HTML to general text will be my first step of data preparation. I will use BeautifulSoup for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whinging. My client&boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&reviewed correctly. \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# for removing html coding in strings of the text\n",
    "text = \"Whinging. My client&amp;boss don't understand English well. Rewrote some text unreadable. It's written by v. good writer&amp;reviewed correctly. \"\n",
    "example1 = BeautifulSoup(text, 'lxml')\n",
    "example1\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)',\n",
       " '@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!',\n",
       " '@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!',\n",
       " '@97sides CONGRATS :)',\n",
       " 'yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp = [(t) for t in ts.strings(\"positive_tweets.json\")]\n",
    "dfp[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ - handle\n",
    "very Twitter user has a unique username. Any thing directed towards that user can be indicated be writing their username preceded by â@â. Thus, these are like proper nouns. For example, @Apple\n",
    "\n",
    "Regular Expression: @(\\w+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday    for being top engaged members in my community this week :)\n",
      " Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      " we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      " CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in range(5):\n",
    "    print(re.sub(r'@[_A-Za-z0-9]+','', dfp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL links\n",
    "The third part of the cleaning is dealing with URL links, same with @mention, even though it carries some information, for sentiment analysis purpose, this can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in range(5):\n",
    "    print(re.sub('https?://[A-Za-z0-9./]+','',dfp[i]))\n",
    "# Regular Expression: (http|https|ftp)://[a-zA-Z0-9\\\\./]+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag\n",
    "A hashtag is a word or an un-spaced phrase prefixed with the hash symbol (#). These are used to both naming subjects and phrases that are currently in trending topics. For example, #iPad, #news\n",
    "\n",
    "`Regular Expression: #(\\w+)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_only = re.sub(\"#(\\w+)\", \" \", dfp[0])\n",
    "letters_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoticons\n",
    "Use of emoticons is very prevalent throughout the web, more so on micro- blogging sites. We identify the following emoticons and replace them with a single word. Table 4 lists the emoticons we are currently detecting. All other emoticons would be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy emoticons are different versions of smiling face, like \":)\", \":-)\", \": )\", \":D\", \"=)\" etc. \n",
    "\n",
    "Sad emoticons include frowns, like \":(\", \":-(\", \":(\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    " \n",
    "# all emoticons (happy + sad)\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating Characters\n",
    "People often use repeating characters while using colloquial language, like \"Iâm in a hurrryyyyy\", \"We won, yaaayyyyy!\" As our final pre-processing step, we replace characters repeating more than twice as two characters.\n",
    "\n",
    "`Regular Expression: (.)\\1{1,}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuations\n",
    "Although not all Punctuations are important from the point of view of classification but some of these, like question mark, exclamation mark can also provide information about the sentiments of the text. We replace every word boundary by a list of relevant punctuations present at that point. Table 5 lists the punctuations currently identified. We also remove any single quotes that might exist in the text.\n",
    "\n",
    "`Punctuations\tExamples\n",
    " PUNC_DOT\t     .\t\n",
    " PUNC_EXCL\t    !\t   Â¡\n",
    " PUNC_QUES\t    ?\t   Â¿\n",
    " PUNC_ELLP\t    ...     â¦`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Tweets\n",
    "NLTK has a TweetTokenizer module that does a good job in tokenizing (splitting text into a list of words) tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different parameters can be passed while calling the TweetTokenizer class. They are:\n",
    "\n",
    "- preserve_case: if False then it converts tweet to lowercase and vice-versa.\n",
    "- strip_handles: if True then it removes twitter handles from the tweet and vice-versa.\n",
    "- reduce_len: if True then it reduces the length of words in the tweet like hurrayyyy, yipppiieeee, etc. and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!']\n",
      "['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!']\n",
      "['congrats', ':)']\n",
      "['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    " \n",
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet_tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "- Remove stock market tickers like $GE\n",
    "- Remove retweet text âRTâ\n",
    "- Remove hyperlinks\n",
    "- Remove hashtags (only the hashtag # and not the word)\n",
    "- Remove stop words like a, and, the, is, are, etc.\n",
    "- Remove emoticons like :), :D, :(, :-), etc.\n",
    "- Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "- Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like âworkingâ, âworksâ, and âworkedâ will be converted to their base/stem word âworkâ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing required libraries\n",
    "import string\n",
    "import re\n",
    " \n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    \n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)           ## remove stock market tickers like $GE Â£GE\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)        ## remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)    ## remove hyperlinks\n",
    "    tweet = re.sub(r'#', '', tweet)               ## remove hashtags sign from the word\n",
    "    tweet = re.sub(r'\\d+','', tweet)              ## remove numbers\n",
    "    tweet = re.sub(r'[.][.][.]','', tweet)        ## remove special punctuations\n",
    "    tweet = re.sub(r'[.][ ][.][ ][.]','', tweet)  ## remove special punctuations\n",
    "    tweet = re.sub(r'(.\\n.)','', tweet)           ## remove special punctuations\n",
    "    tweet = re.sub(r'[â] | [â]','', tweet)        ## remove special punctuations\n",
    "    tweet = re.sub(r'@[_A-Za-z0-9]+','', tweet)   ## remove handlers\n",
    "    tweet = re.sub(r'_','', tweet)  ## remove special punctuations\n",
    "    tweet = tweet.replace('\\n', '')               ## replace new line html decoder\n",
    "    \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and       ## remove stopwords\n",
    "              word not in emoticons and             ## remove emoticons\n",
    "                word not in string.punctuation):    ## remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            words_lemma = lemmatizer.lemmatize(word) ## lemmatizer\n",
    "            #stem_word = stemmer.stem(word)          ## stemming word\n",
    "            tweets_clean.append(words_lemma)\n",
    "            \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'great', 'day', 'good', 'morning']\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    " \n",
    "# print cleaned tweet\n",
    "print (clean_tweets(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "['one', 'irresistible', 'lipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "print (pos_tweets[5])\n",
    "\n",
    "# print cleaned tweet\n",
    "print (clean_tweets(pos_tweets[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('@GrahamTownsend More like an Ewok in my case :D @MiamiTrue', 'pos')\n",
      "['like', 'ewok', 'case']\n"
     ]
    }
   ],
   "source": [
    "print (ttrain[5])\n",
    "\n",
    "# print cleaned tweet\n",
    "print (clean_tweets(ttrain[5][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets unpack the train data and apply above pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_train_text = []\n",
    "for a, b in ttrain:\n",
    "    #print(a)\n",
    "    df_pos_train_text.append(a)\n",
    "    \n",
    "len(df_pos_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'n', 'america', 'california', 'usa', 'dear', 'friend', 'thank', 'following']\n"
     ]
    }
   ],
   "source": [
    "print (clean_tweets(df_pos_train_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_set = []\n",
    "for tweet in range(len(df_pos_train_text)):\n",
    "    clean_tweets_set.append((clean_tweets(df_pos_train_text[tweet])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello',\n",
       "  'n',\n",
       "  'america',\n",
       "  'california',\n",
       "  'usa',\n",
       "  'dear',\n",
       "  'friend',\n",
       "  'thank',\n",
       "  'following'],\n",
       " ['stuff', 'well', 'would', 'know'],\n",
       " ['hate', 'waking', 'middle', 'sleep'],\n",
       " ['he',\n",
       "  'adorable',\n",
       "  'like',\n",
       "  'cute',\n",
       "  'marshmallow',\n",
       "  'u',\n",
       "  'wanna',\n",
       "  'hug',\n",
       "  'eat',\n",
       "  'bc',\n",
       "  'cute',\n",
       "  'chswiyfxcskcalum',\n",
       "  'love',\n",
       "  'much'],\n",
       " ['jgh', 'bonding', 'niggs', 'bonakid'],\n",
       " ['like', 'ewok', 'case'],\n",
       " ['followfriday', 'top', 'support', 'community', 'week'],\n",
       " ['call', 'savage', 'god'],\n",
       " ['bath', 'even', 'â', 'wheelie', 'bin', 'nowhere', 'put'],\n",
       " ['stats', 'day', 'arrived', 'new', 'follower', 'unfollowers', 'via']]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'n',\n",
       " 'america',\n",
       " 'california',\n",
       " 'usa',\n",
       " 'dear',\n",
       " 'friend',\n",
       " 'thank',\n",
       " 'following',\n",
       " 'stuff',\n",
       " 'well',\n",
       " 'would',\n",
       " 'know',\n",
       " 'hate',\n",
       " 'waking',\n",
       " 'middle',\n",
       " 'sleep',\n",
       " 'he',\n",
       " 'adorable',\n",
       " 'like',\n",
       " 'cute',\n",
       " 'marshmallow',\n",
       " 'u',\n",
       " 'wanna',\n",
       " 'hug',\n",
       " 'eat',\n",
       " 'bc',\n",
       " 'cute',\n",
       " 'chswiyfxcskcalum',\n",
       " 'love',\n",
       " 'much',\n",
       " 'jgh',\n",
       " 'bonding',\n",
       " 'niggs',\n",
       " 'bonakid',\n",
       " 'like',\n",
       " 'ewok',\n",
       " 'case',\n",
       " 'followfriday',\n",
       " 'top',\n",
       " 'support',\n",
       " 'community',\n",
       " 'week',\n",
       " 'call',\n",
       " 'savage',\n",
       " 'god',\n",
       " 'bath',\n",
       " 'even',\n",
       " 'â',\n",
       " 'wheelie']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will make above list of list into flat list\n",
    "flat_list = [item for sublist in clean_tweets_set for item in sublist]\n",
    "flat_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Bag-of-words model that allows us to represent text as numerical feature vectors. \n",
    "\n",
    "The idea behind the bag-of-words model is quite simple and can be summarized as follows:\n",
    "1. We create a vocabulary of unique tokensâfor example, wordsâfrom the entire set of documents.\n",
    "2. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6Vf3cSwjNaV"
   },
   "source": [
    "### Vocabulary\n",
    "This function will create a vocabulary for each review and use it to get unigram features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9301\n",
      "['(-:', '(:', '):', ');', '--->', '-->', '->', '. .', '. ..', '..', \":'D\", ':-\\\\', ':/', '::', ':op', ':|', ';-)', ';d', ';p', ';}', '<--', '<---', '=:', '>:-(', '>:D', 'D:', 'D;', 'a-foot', 'aa', 'aaa', 'aaaaaaaaah', 'aaaahhh', 'aaaand', 'aaah', 'aaahh', 'aaahhh', 'aahhh', 'aameell', 'aapke', 'aaron', 'aarwwi', 'aasramany', 'aayegi', 'ab', 'abandoned', 'abby', 'abby.can', 'abbymill', 'abes', 'abhi', 'abi', 'abit', 'able', 'abligaverins', 'abp', 'abroad', 'abrupt', 'abscess', 'absolute', 'absolutely', 'abstinence', 'abt', 'abu', 'abuti', 'ac', 'academic', 'acads', 'acc', 'accent', 'access', 'accessory', 'accident', 'accidentally', 'accnt', 'accomplishment', 'according', 'account', 'accurate', 'acdc', 'acha', 'achebe', 'achi', 'achieved', 'achieving', 'ack', 'acne', 'acorn', 'acoustic', 'across', 'act', 'action', 'activation', 'active', 'activity', 'actor', 'actress', 'actual', 'actually', 'actuallythough', 'acube', 'ad', 'ada', 'adam', 'add', 'added', 'addicted', 'addicted-to-analsex', 'addiction', 'addictive', 'adding', 'addme', 'addmeonbbm', 'addmeonsnapchat', 'address', 'addressing', 'adf', 'adidas', 'adios', 'aditya', 'adolf', 'adorable', 'adore', 'adored', 'adrian', 'adult', 'advaddict', 'advance', 'advanced', 'adventure', 'adventurous', 'advert', 'advice', 'aerial', 'af', 'afang', 'aff', 'affaraid', 'affect', 'afford', 'afghanistn', 'aflblueshawks', 'afraid', 'afridi', 'afropunk', 'afsos', 'aftenoonfeeling', 'afterall', 'afternon', 'afternoon', 'afterschool', 'afterwards', 'afterznoon', 'aftie', 'agaaain', 'againawalmu', 'agains', 'agayhippiehippy', 'age', 'aged', 'agency', 'agh', 'agnes', 'agnezmo', 'ago', 'agonising', 'agover', 'agree', 'agreed', 'agrees', 'ah', 'aha', 'ahaha', 'ahahah', 'ahahha', 'ahead', 'ahh', 'ahhh', 'ahmad', \"ahmi've\", 'ahourÃ©', 'ahs', 'aich', 'aigh', 'aigoo', 'aigooo', 'aimed', \"ain't\", 'aing', 'aint', 'air', 'airdroid', 'aired', 'airforce', 'airline', 'airport', 'aishhh', 'aisyah', 'aisyhhh', 'ait', 'aitor', 'aj', 'aja', 'ajunchicken', 'akana', 'akarshan', 'ake', 'akere', 'aki', 'akit', 'ako', 'akong', 'akooo', 'akshaymostlovedsuperstarever', 'aku', 'akua', 'al', 'ala', 'alacer', 'alarm', 'albanian', 'albay', 'alberta', 'album', 'alchemist', 'alcohol', 'aldub', 'alert', 'alex', 'algorithm', 'alhamdulillah', 'alhamdullilah', 'alice', 'alien', 'alison', 'alive', 'alix', 'allah', 'allergic', 'allgoodthingske', 'alliteration', 'alll', 'allover', 'allow', 'allowance', 'allowed', 'allowing', 'allows', 'allpawonhungryhoundddwalkhealthyhoundspawsforawalk', 'alls', 'ally', 'alma', 'almost', 'alone', 'along', 'alot', 'alrd', 'alreaddyyy', 'already', 'alright', 'also', 'alternative', 'alternatively', 'although', 'aluminiumfree', 'alumnus', 'alunageorge', 'alute', 'always', 'alwayskeepfighting', 'amat', 'amateur', 'amazed', 'amazes', 'amazing', 'amazon', 'amber', 'ambrose', 'amelia', 'amen', 'america', 'american', 'americano', 'ami', 'amiibo', 'aminn', 'amm', 'amnotness', 'among', 'amount', 'ampsha', 'amsterdam', 'amtired', 'amusement', 'amy', 'amyjackson', 'an', 'anaged', 'analyst', 'analyzing', 'anarchy', 'anatomy', 'andals', 'andaming', 'andekjs', 'andi', 'andre', 'andrew', 'android', 'andromeda', 'andy', 'andyhank', 'anesthesia', 'ang', 'angeke', 'angel', 'angelo', \"angelo's\", 'anger', 'angie', 'angle.nelson', 'angry', 'angry.now', 'animal', 'animated', 'animation', 'animator', 'anime', 'anjustinbieber', 'ank', 'ankle', 'ann', 'anna', 'anne', 'anniversary', 'announce', 'announcement', 'annoyed', 'annoying', 'annyeong', 'ano', 'anonymous', 'anot', 'another', 'answer', 'answered', 'answering', 'answin', 'ant', 'antagal', 'antagonistic', 'anti-christ', 'antonio', 'anu', 'anvy', 'anxiety', 'anxious', 'anyare', 'anybody', 'anymore', 'anymoreee', 'anyone', 'anyones', 'anythin', 'anything', 'anything.surely', 'anytime', 'anyway', 'anywayhedidanicejob', 'anyways', 'anywhere', 'anz', 'ap', 'apaghimok', 'apart', 'apartment', 'apb', 'apcom', 'ape', 'apexis', 'apink', \"apink's\", 'aplomb', \"apma's\", 'apmas', 'apod', 'apologizing', 'apology', 'app', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appears', 'appendicitis', 'appendix', 'apple', 'application', 'apply', 'appointment', 'appreciate', 'appreciated', 'appreciating', 'appropriate', 'approval', 'approve', 'approving', 'apps', 'appy', 'appyfriday', 'april', 'aps', 'aquatic', 'arabia', 'araw', 'arbeloa', 'arch', 'archdbanterbury', 'architecture', 'arctic', 'ardent', 'area', 'aready', 'areal', 'arent', 'argh', 'argue', 'argument', 'ariana', 'arianator', 'arianna', 'arm', 'arma', 'armonrae', 'armor', 'arnd', 'arond', 'around', 'arrange', 'arre', 'arresting', 'arrive', 'arrived', 'arsenal', 'art', 'article', 'artist', 'artistic', 'artworkbylie', 'arummzz', 'as', 'asap', 'asf', 'ashamed', 'ashraf', 'ashramcalling', 'ashwathy', 'asia', 'asian', 'asianbut', 'ask', 'askaman', 'asked', 'askfinnick', 'askies', 'asking', 'askip', 'askurban', 'asleep', 'aspect', 'asshole', 'assia', 'assignment', 'assingnment', 'associate', 'assume', 'assured', 'asthma', 'atchya', 'ate', 'athabasca', \"athena's\", 'atk', 'atlantis', 'atlas', 'atleast', 'atm', 'atp', 'atrack', 'atrocity', 'attached', 'attack', 'attacked', 'attempt', 'attend', 'attender', 'attention', 'attentive', 'attitude', \"attitude's\"]\n"
     ]
    }
   ],
   "source": [
    "#Required for Bag of words (unigram features) creation\n",
    "vocabulary = list(set(flat_list))\n",
    "vocabulary.sort() #sorting the list\n",
    "print(len(vocabulary))\n",
    "print(vocabulary[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dlxw58jcKvfE"
   },
   "source": [
    "### Unigram Features\n",
    "This function will get unigram features from the data.<br>\n",
    "Prepare a unigram feature vector based on the presence or absence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wu9MqvTwjNad"
   },
   "outputs": [],
   "source": [
    "def get_unigram_features(data, vocab):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        single_feat_vec = []\n",
    "        sent = tup[0].lower()                  ## Lowercasing the dataset\n",
    "        for v in vocab:\n",
    "            if sent.__contains__(v):\n",
    "                single_feat_vec.append(1)\n",
    "            else:\n",
    "                single_feat_vec.append(0)\n",
    "        fet_vec_all.append(single_feat_vec)\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1523490988607,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "uHtP7RQBsZJL",
    "outputId": "684a3779-24b8-4899-9a9a-dc71df1ed3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "## code for printing time in seconds for execution of each cell\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  ## time consumed = start time - end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65238,
     "status": "ok",
     "timestamp": 1523491683799,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "QC8-UDv6XrLt",
    "outputId": "bf165261-9607-4a2a-f783-3077ef5a7d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 31.500887632369995 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_unigram_features = get_unigram_features(ttrain, vocabulary)                      # vocabulary extracted in the beginning\n",
    "test_unigram_features = get_unigram_features(ttest, vocabulary)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zONG459eXrLw"
   },
   "source": [
    "### Sentiword net\n",
    "Lets add more features from sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 928,
     "status": "ok",
     "timestamp": 1523491313275,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "0YAURRClZlcw",
    "outputId": "6d4ffea8-e894-465b-dbed-e94922c97e58",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CorpusReader.abspath of <SentiWordNetCorpusReader in 'C:\\\\Users\\\\Sai\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\sentiwordnet'>>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "#nltk.download(\"all\")\n",
    "swn.abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n421k-ncs7tn"
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import sentiwordnet as swn\n",
    "# nltk.download(\"sentiwordnet\")\n",
    "\n",
    "def get_senti_wordnet_features(data):\n",
    "    fet_vec_all = []\n",
    "    for tup in data:\n",
    "        sent = tup[0].lower()\n",
    "        words = sent.split()\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for w in words:\n",
    "            senti_synsets = swn.senti_synsets(w.lower())\n",
    "            for senti_synset in senti_synsets:\n",
    "                p = senti_synset.pos_score()\n",
    "                n = senti_synset.neg_score()\n",
    "                pos_score+=p\n",
    "                neg_score+=n\n",
    "                break #take only the first synset (Most frequent sense)\n",
    "        fet_vec_all.append([float(pos_score),float(neg_score)])\n",
    "    return fet_vec_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7541,
     "status": "ok",
     "timestamp": 1523491726744,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "qQayDMYMXrLx",
    "outputId": "6a683d55-06ba-4099-f5da-354fbeca0cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.14002776145935 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_swn_features = get_senti_wordnet_features(ttrain)\n",
    "test_swn_features     = get_senti_wordnet_features(ttest)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhgONImSXrLz"
   },
   "source": [
    "**Merge the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "watuXraZtcl4"
   },
   "outputs": [],
   "source": [
    "def merge_features(featureList1,featureList2):\n",
    "    # For merging two features\n",
    "    if featureList1==[]:\n",
    "        return featureList2\n",
    "    merged = []\n",
    "    for i in range(len(featureList1)):\n",
    "        m = featureList1[i]+featureList2[i]\n",
    "        merged.append(m)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4667,
     "status": "ok",
     "timestamp": 1523491733367,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "ITRZpqtKjNcO",
    "outputId": "5595817b-7c06-40d4-b8db-08214390c008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.196302890777588 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ttraining_features = merge_features( training_unigram_features, training_swn_features)\n",
    "ttest_features     = merge_features( test_unigram_features,     test_swn_features)\n",
    "\n",
    "## These are the final set of features on which training and testing can ber performed\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will calculate the prediction(classification) precision with reference to actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(prediction, actual):\n",
    "    prediction = list(prediction)\n",
    "    correct_labels = [prediction[i]  for i in range(len(prediction)) if actual[i] == prediction[i]]\n",
    "    precision = float(len(correct_labels))/float(len(prediction))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of NB classifier is\n",
      "Training data\t0.8834666666666666\n",
      "Test data\t0.7376\n",
      "--- 14.028931140899658 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB(alpha=0.1).fit(ttraining_features, ttraining_labels) #training process\n",
    "\n",
    "print(\"Precision of NB classifier is\")\n",
    "\n",
    "pred = nb_classifier.predict(ttraining_features)\n",
    "precision = calculate_precision(pred, ttraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = nb_classifier.predict(ttest_features)\n",
    "twitter_nb = calculate_precision(pred, ttest_gold_labels)\n",
    "print(\"Test data\\t\" + str(twitter_nb))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Decision Tree classifier is\n",
      "Training data\t0.5933333333333334\n",
      "Test data\t0.5852\n",
      "--- 13.119225025177002 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=5, max_features='sqrt', random_state=42).fit(ttraining_features, ttraining_labels)\n",
    "\n",
    "print(\"Precision of Decision Tree classifier is\")\n",
    "\n",
    "pred = clf.predict(ttraining_features)\n",
    "precision = calculate_precision(pred, ttraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(ttest_features)\n",
    "twitter_dt = calculate_precision(pred, ttest_gold_labels)\n",
    "print(\"Test data\\t\" + str(twitter_dt))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Logistic Regression is\n",
      "Training data\t0.9290666666666667\n",
      "Test data\t0.758\n",
      "--- 13.952841758728027 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=42).fit(ttraining_features, ttraining_labels)\n",
    "\n",
    "print(\"Precision of Logistic Regression is\")\n",
    "\n",
    "pred = clf.predict(ttraining_features)\n",
    "precision = calculate_precision(pred, ttraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(ttest_features)\n",
    "twitter_lr = calculate_precision(pred, ttest_gold_labels)\n",
    "print(\"Test data\\t\" + str(twitter_lr))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of SVM classifier is\n",
      "Training data\t0.5022857142857143\n",
      "Test data\t0.49466666666666664\n",
      "--- 2708.3680284023285 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=0.01, kernel='sigmoid').fit(ttraining_features, ttraining_labels)\n",
    "\n",
    "print(\"Precision of SVM classifier is\")\n",
    "\n",
    "pred = clf.predict(ttraining_features)\n",
    "precision = calculate_precision(pred, ttraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(mtest_features)\n",
    "twitter_svm = calculate_precision(pred, ttest_gold_labels)\n",
    "print(\"Test data\\t\" + str(twitter_svm))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5FkrkbXOuAJ"
   },
   "source": [
    "# (1) Sentiment Analysis on Movie Review Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cS-D61tjNZo"
   },
   "source": [
    "**Task 1:** The goal here is to predict the sentiments behind movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_v4xh6a2soXy"
   },
   "source": [
    "# (1) Using \"movie_reviews\" from nltk\n",
    "### movie_reviews corpus : \n",
    "- 2000 movie reviews which are stored in separate text files with sentiment polarity classification tag.\n",
    "   <br>for eg. neg/cv000_29416.txt', 'pos/cv000_29590.txt'\n",
    "- This corpus contains two categories of text: pos (a positive sentiment) and neg (a negative sentiment). \n",
    "- First 1000 reviews are negative and later are positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 4369
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65321,
     "status": "ok",
     "timestamp": 1523490949310,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "j9JX5eh1wY7t",
    "outputId": "5e32dd41-7111-43f5-e006-98aeafb540f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CorpusReader.abspath of <CategorizedPlaintextCorpusReader in 'C:\\\\Users\\\\Sai\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\movie_reviews'>>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                                     ## import Natural Language Tool Kit\n",
    "#nltk.download(\"all\")                           ## download all the packages and text files from nltk\n",
    "#nltk.download(\"movie_reviews\")                 ## download and unzip all the text files of movie_reviews\n",
    "from nltk.corpus import movie_reviews as mr     ## import movie_review text files as mr\n",
    "mr.abspath                                      ## location of files where they are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "pppzQD2I0Idz"
   },
   "outputs": [],
   "source": [
    "# mr.fileids(\"pos\")                             ## list of files with positive sentiment\n",
    "# mr.fileids(\"neg\")                             ## list of files with negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwnogMwwjNZy"
   },
   "source": [
    "## Creating a categorized text corpus :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzGvzqLdjNZ0"
   },
   "source": [
    "For training, we need to first create a list of labeled feature sets.<br>\n",
    "\n",
    "We will convert the Categorized Plaintext Corpus Reader data into python list.<br>\n",
    "\n",
    "This list should be of the form [(featureset, label)], where the featureset variable is a dict and label is the known class label for the featureset.<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1092,
     "status": "ok",
     "timestamp": 1523490964834,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "u1Jr0q83jNZ1",
    "outputId": "1d08e6cd-f3c3-4202-fcc3-675d7151ed9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = []                              ## create an empty list\n",
    "\n",
    "for fileid in mr.fileids():\n",
    "    tag, filename = fileid.split('/')     ## extracting featureset(text) and its corresponding labels from file name\n",
    "    reviews.append([mr.raw(fileid), tag]) ## appending the featureset to the list in the form of [(featureset, label)]\n",
    "    \n",
    "len(reviews)                              ## length of list created; 0-999 = neg, 1000-1999 = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0UElu0_42iaL"
   },
   "outputs": [],
   "source": [
    "#print(reviews[999])                       ## sample of negative sentiment review list\n",
    "#print(reviews[1000])                      ## sample of positive sentiment review list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L2vr4r_739QC"
   },
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCyJobEQ4bg5"
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fMaFo42cjNaC"
   },
   "source": [
    "In our dataset first 1000 reviews are negative and later are positives. So we will shuffle the data using shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "h_rf8pLtjNaD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)                          ## for reproducing same results each time\n",
    "\n",
    "\n",
    "from random import shuffle                ## import required package/ functions\n",
    "shuffle(reviews)                          ## shuffle the ordered reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AcJRA0ikjNaQ"
   },
   "source": [
    "Now we divide the dataset into train and test.<br>\n",
    "- Train (75%) data will be processed in order to use it for training a classification model<br>\n",
    "- Test (25%) data will be used for predicting the sentiment out of the review by the model built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1017,
     "status": "ok",
     "timestamp": 1523490972175,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "mjG2rDBojNaR",
    "outputId": "c970cedd-d2ff-4307-b40c-59fe61a6260e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 : 500\n"
     ]
    }
   ],
   "source": [
    "mtrain = reviews[:1500]                    ## 75% of the total data\n",
    "mtest  = reviews[1500:]                    ## remaining 25% of the total data\n",
    "print(len(mtrain),\":\",len(mtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmLTycNo628i"
   },
   "source": [
    "<space>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V9o6m6Zt62BG"
   },
   "source": [
    "# (2) Processing the data :\n",
    "- We have to convert categorical data, such as text or words, into a numerical form before we can pass it on to a machine learning algorithm.<br>\n",
    "- We will use training data as reference for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9qG07QUosRoK"
   },
   "outputs": [],
   "source": [
    "mtraining_labels = get_lables(mtrain)\n",
    "\n",
    "mtest_gold_labels = get_lables(mtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##len(mtraining_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets build bag of words from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will remove the tags from training data, only text will be stores in df\n",
    "df = []\n",
    "for a, b in mtrain:\n",
    "    #print(a)\n",
    "    df.append(a)\n",
    "    \n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Html decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'most movies seem to release a third movie just so it can be called a trilogy . \\nrocky iii seems to kind of fit in that category , but manages to be slightly unique . \\nthe rocky formula of \" rocky loses fight/rocky trains/rocky wins fight \" is carried out to the letter . \\nalso the \\'tradition\\' of showing the last five minutes or so from the past rocky film is used as well . \\nthis movie begins with a series of clips showing how famous rocky ( sylvester stallone ) has become . . . \\neven showing a brief appearance on sesame street ! ! \\nthen it moves on with rocky being in a fixed fight with thunderlips ( hulk hogan ) . \\na mysterious bad-ass known as clubber lang ( mr . t ) trash-talks to rocky about his stupid decision to retire from boxing and that he \" pities the fool \" for not coming out and fighting him outright . \\nrocky\\'s trainer ( burgess meredith ) tells rocky not to fight , but the italian stallion doesn\\'t listen . \\nnaturally , he gets his ass kicked . \\nsomewhere along the line after this several things happen . \\nrocky\\'s longtime trainer dies , causing rocky to train with his former opponent apollo creed . \\nrocky\\'s wife complains to her husband that he should fight . \\nthe final fight ensues between clubber and rocky . \\nguess who wins ? \\nthe winner\\'s name rhymes with \\'smocky\\' . \\nthe movie is entertaning mainly because of clubber lang\\'s over the top performance . \\nthe dramatic aspect has been toned down considerably since rocky ii and now action seems to be the strong point of the film . \\nwhich is good if you like fighting scenes . \\nthe last match is quite decent actually . \\nif you liked the previous films , rent this one . \\nit\\'s well worth seeing if you are a fan of the series . \\nbut if you can\\'t stand rocky shouting \" adrianne ! ! ! \" \\none more time , see something else . \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most movies seem to release a third movie just so it can be called a trilogy . \n",
      "rocky iii seems to kind of fit in that category , but manages to be slightly unique . \n",
      "the rocky formula of \" rocky loses fight/rocky trains/rocky wins fight \" is carried out to the letter . \n",
      "also the 'tradition' of showing the last five minutes or so from the past rocky film is used as well . \n",
      "this movie begins with a series of clips showing how famous rocky ( sylvester stallone ) has become . . . \n",
      "even showing a brief appearance on sesame street ! ! \n",
      "then it moves on with rocky being in a fixed fight with thunderlips ( hulk hogan ) . \n",
      "a mysterious bad-ass known as clubber lang ( mr . t ) trash-talks to rocky about his stupid decision to retire from boxing and that he \" pities the fool \" for not coming out and fighting him outright . \n",
      "rocky's trainer ( burgess meredith ) tells rocky not to fight , but the italian stallion doesn't listen . \n",
      "naturally , he gets his ass kicked . \n",
      "somewhere along the line after this several things happen . \n",
      "rocky's longtime trainer dies , causing rocky to train with his former opponent apollo creed . \n",
      "rocky's wife complains to her husband that he should fight . \n",
      "the final fight ensues between clubber and rocky . \n",
      "guess who wins ? \n",
      "the winner's name rhymes with 'smocky' . \n",
      "the movie is entertaning mainly because of clubber lang's over the top performance . \n",
      "the dramatic aspect has been toned down considerably since rocky ii and now action seems to be the strong point of the film . \n",
      "which is good if you like fighting scenes . \n",
      "the last match is quite decent actually . \n",
      "if you liked the previous films , rent this one . \n",
      "it's well worth seeing if you are a fan of the series . \n",
      "but if you can't stand rocky shouting \" adrianne ! ! ! \" \n",
      "one more time , see something else . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# for removing html coding in strings of the text\n",
    "example1 = BeautifulSoup(df[0], 'lxml')\n",
    "example1\n",
    "print(example1.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "- Remove stock market tickers like $GE\n",
    "- Remove hyperlinks\n",
    "- Remove hashtags (only the hashtag # and not the word)\n",
    "- Remove stop words like a, and, the, is, are, etc.\n",
    "- Remove emoticons like :), :D, :(, :-), etc.\n",
    "- Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "- Convert words to Stem/Base words using nltk lemmatizer. E.g. words like âworkingâ, âworksâ, and âworkedâ will be converted to their base/stem word âworkâ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words_set = []\n",
    "for word in range(len(df)):\n",
    "    clean_words_set.append((clean_tweets(df[word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'seem',\n",
       " 'release',\n",
       " 'third',\n",
       " 'movie',\n",
       " 'called',\n",
       " 'trilogy',\n",
       " 'rocky',\n",
       " 'iii',\n",
       " 'seems',\n",
       " 'kind',\n",
       " 'fit',\n",
       " 'category',\n",
       " 'manages',\n",
       " 'slightly',\n",
       " 'unique',\n",
       " 'rocky',\n",
       " 'formula',\n",
       " 'rocky',\n",
       " 'loses',\n",
       " 'fight',\n",
       " 'rocky',\n",
       " 'train',\n",
       " 'rocky',\n",
       " 'win',\n",
       " 'fight',\n",
       " 'carried',\n",
       " 'letter',\n",
       " 'also',\n",
       " 'tradition',\n",
       " 'showing',\n",
       " 'last',\n",
       " 'five',\n",
       " 'minute',\n",
       " 'past',\n",
       " 'rocky',\n",
       " 'film',\n",
       " 'used',\n",
       " 'well',\n",
       " 'movie',\n",
       " 'begin',\n",
       " 'series',\n",
       " 'clip',\n",
       " 'showing',\n",
       " 'famous',\n",
       " 'rocky',\n",
       " 'sylvester',\n",
       " 'stallone',\n",
       " 'become',\n",
       " '. . .',\n",
       " 'even',\n",
       " 'showing',\n",
       " 'brief',\n",
       " 'appearance',\n",
       " 'sesame',\n",
       " 'street',\n",
       " 'move',\n",
       " 'rocky',\n",
       " 'fixed',\n",
       " 'fight',\n",
       " 'thunderlips',\n",
       " 'hulk',\n",
       " 'hogan',\n",
       " 'mysterious',\n",
       " 'bad-ass',\n",
       " 'known',\n",
       " 'clubber',\n",
       " 'lang',\n",
       " 'mr',\n",
       " 'trash-talks',\n",
       " 'rocky',\n",
       " 'stupid',\n",
       " 'decision',\n",
       " 'retire',\n",
       " 'boxing',\n",
       " 'pity',\n",
       " 'fool',\n",
       " 'coming',\n",
       " 'fighting',\n",
       " 'outright',\n",
       " \"rocky's\",\n",
       " 'trainer',\n",
       " 'burgess',\n",
       " 'meredith',\n",
       " 'tell',\n",
       " 'rocky',\n",
       " 'fight',\n",
       " 'italian',\n",
       " 'stallion',\n",
       " 'listen',\n",
       " 'naturally',\n",
       " 'get',\n",
       " 'as',\n",
       " 'kicked',\n",
       " 'somewhere',\n",
       " 'along',\n",
       " 'line',\n",
       " 'several',\n",
       " 'thing',\n",
       " 'happen',\n",
       " \"rocky's\",\n",
       " 'longtime',\n",
       " 'trainer',\n",
       " 'dy',\n",
       " 'causing',\n",
       " 'rocky',\n",
       " 'train',\n",
       " 'former',\n",
       " 'opponent',\n",
       " 'apollo',\n",
       " 'creed',\n",
       " \"rocky's\",\n",
       " 'wife',\n",
       " 'complains',\n",
       " 'husband',\n",
       " 'fight',\n",
       " 'final',\n",
       " 'fight',\n",
       " 'ensues',\n",
       " 'clubber',\n",
       " 'rocky',\n",
       " 'guess',\n",
       " 'win',\n",
       " \"winner's\",\n",
       " 'name',\n",
       " 'rhyme',\n",
       " 'smocky',\n",
       " 'movie',\n",
       " 'entertaning',\n",
       " 'mainly',\n",
       " 'clubber',\n",
       " \"lang's\",\n",
       " 'top',\n",
       " 'performance',\n",
       " 'dramatic',\n",
       " 'aspect',\n",
       " 'toned',\n",
       " 'considerably',\n",
       " 'since',\n",
       " 'rocky',\n",
       " 'ii',\n",
       " 'action',\n",
       " 'seems',\n",
       " 'strong',\n",
       " 'point',\n",
       " 'film',\n",
       " 'good',\n",
       " 'like',\n",
       " 'fighting',\n",
       " 'scene',\n",
       " 'last',\n",
       " 'match',\n",
       " 'quite',\n",
       " 'decent',\n",
       " 'actually',\n",
       " 'liked',\n",
       " 'previous',\n",
       " 'film',\n",
       " 'rent',\n",
       " 'one',\n",
       " 'well',\n",
       " 'worth',\n",
       " 'seeing',\n",
       " 'fan',\n",
       " 'series',\n",
       " \"can't\",\n",
       " 'stand',\n",
       " 'rocky',\n",
       " 'shouting',\n",
       " 'adrianne',\n",
       " 'one',\n",
       " 'time',\n",
       " 'see',\n",
       " 'something',\n",
       " 'else']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words(df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'seem',\n",
       " 'release',\n",
       " 'third',\n",
       " 'movie',\n",
       " 'called',\n",
       " 'trilogy',\n",
       " 'ocky',\n",
       " 'iii',\n",
       " 'seems',\n",
       " 'kind',\n",
       " 'fit',\n",
       " 'category',\n",
       " 'manages',\n",
       " 'slightly',\n",
       " 'unique',\n",
       " 'rocky',\n",
       " 'formula',\n",
       " 'rocky',\n",
       " 'loses',\n",
       " 'fight',\n",
       " 'rocky',\n",
       " 'train',\n",
       " 'rocky',\n",
       " 'win',\n",
       " 'fight',\n",
       " 'carried',\n",
       " 'letter',\n",
       " 'lso',\n",
       " 'tradition',\n",
       " 'showing',\n",
       " 'last',\n",
       " 'five',\n",
       " 'minute',\n",
       " 'past',\n",
       " 'rocky',\n",
       " 'film',\n",
       " 'used',\n",
       " 'well',\n",
       " 'movie',\n",
       " 'begin',\n",
       " 'series',\n",
       " 'clip',\n",
       " 'showing',\n",
       " 'famous',\n",
       " 'rocky',\n",
       " 'sylvester',\n",
       " 'stallone',\n",
       " 'become',\n",
       " 'ven',\n",
       " 'showing',\n",
       " 'brief',\n",
       " 'appearance',\n",
       " 'sesame',\n",
       " 'street',\n",
       " 'hen',\n",
       " 'move',\n",
       " 'rocky',\n",
       " 'fixed',\n",
       " 'fight',\n",
       " 'thunderlips',\n",
       " 'hulk',\n",
       " 'hogan',\n",
       " 'mysterious',\n",
       " 'bad-ass',\n",
       " 'known',\n",
       " 'clubber',\n",
       " 'lang',\n",
       " 'mr',\n",
       " 'trash-talks',\n",
       " 'rocky',\n",
       " 'stupid',\n",
       " 'decision',\n",
       " 'retire',\n",
       " 'boxing',\n",
       " 'pity',\n",
       " 'fool',\n",
       " 'coming',\n",
       " 'fighting',\n",
       " 'outright',\n",
       " \"ocky's\",\n",
       " 'trainer',\n",
       " 'burgess',\n",
       " 'meredith',\n",
       " 'tell',\n",
       " 'rocky',\n",
       " 'fight',\n",
       " 'italian',\n",
       " 'stallion',\n",
       " 'listen',\n",
       " 'aturally',\n",
       " 'get',\n",
       " 'as',\n",
       " 'kicked',\n",
       " 'omewhere',\n",
       " 'along',\n",
       " 'line',\n",
       " 'several',\n",
       " 'thing',\n",
       " 'happen',\n",
       " \"ocky's\",\n",
       " 'longtime',\n",
       " 'trainer',\n",
       " 'dy',\n",
       " 'causing',\n",
       " 'rocky',\n",
       " 'train',\n",
       " 'former',\n",
       " 'opponent',\n",
       " 'apollo',\n",
       " 'creed',\n",
       " \"ocky's\",\n",
       " 'wife',\n",
       " 'complains',\n",
       " 'husband',\n",
       " 'fight',\n",
       " 'final',\n",
       " 'fight',\n",
       " 'ensues',\n",
       " 'clubber',\n",
       " 'rocky',\n",
       " 'uess',\n",
       " 'win',\n",
       " \"winner's\",\n",
       " 'name',\n",
       " 'rhyme',\n",
       " 'smocky',\n",
       " 'movie',\n",
       " 'entertaning',\n",
       " 'mainly',\n",
       " 'clubber',\n",
       " \"lang's\",\n",
       " 'top',\n",
       " 'performance',\n",
       " 'dramatic',\n",
       " 'aspect',\n",
       " 'toned',\n",
       " 'considerably',\n",
       " 'since',\n",
       " 'rocky',\n",
       " 'ii',\n",
       " 'action',\n",
       " 'seems',\n",
       " 'strong',\n",
       " 'point',\n",
       " 'film',\n",
       " 'hich',\n",
       " 'good',\n",
       " 'like',\n",
       " 'fighting',\n",
       " 'scene',\n",
       " 'last',\n",
       " 'match',\n",
       " 'quite',\n",
       " 'decent',\n",
       " 'actually',\n",
       " 'f',\n",
       " 'liked',\n",
       " 'previous',\n",
       " 'film',\n",
       " 'rent',\n",
       " 'one',\n",
       " \"t's\",\n",
       " 'well',\n",
       " 'worth',\n",
       " 'seeing',\n",
       " 'fan',\n",
       " 'series',\n",
       " 'ut',\n",
       " \"can't\",\n",
       " 'stand',\n",
       " 'rocky',\n",
       " 'shouting',\n",
       " 'adrianne',\n",
       " 'ne',\n",
       " 'time',\n",
       " 'see',\n",
       " 'something',\n",
       " 'else']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'seem',\n",
       " 'release',\n",
       " 'third',\n",
       " 'movie',\n",
       " 'called',\n",
       " 'trilogy',\n",
       " 'ocky',\n",
       " 'iii',\n",
       " 'seems',\n",
       " 'kind',\n",
       " 'fit',\n",
       " 'category',\n",
       " 'manages',\n",
       " 'slightly',\n",
       " 'unique',\n",
       " 'rocky',\n",
       " 'formula',\n",
       " 'rocky',\n",
       " 'loses',\n",
       " 'fight',\n",
       " 'rocky',\n",
       " 'train',\n",
       " 'rocky',\n",
       " 'win',\n",
       " 'fight',\n",
       " 'carried',\n",
       " 'letter',\n",
       " 'lso',\n",
       " 'tradition',\n",
       " 'showing',\n",
       " 'last',\n",
       " 'five',\n",
       " 'minute',\n",
       " 'past',\n",
       " 'rocky',\n",
       " 'film',\n",
       " 'used',\n",
       " 'well',\n",
       " 'movie',\n",
       " 'begin',\n",
       " 'series',\n",
       " 'clip',\n",
       " 'showing',\n",
       " 'famous',\n",
       " 'rocky',\n",
       " 'sylvester',\n",
       " 'stallone',\n",
       " 'become',\n",
       " 'ven']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code will make above list of list into flat list\n",
    "flat_list = [item for sublist in clean_words_set for item in sublist]\n",
    "flat_list[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "\n",
    "Bag-of-words model that allows us to represent text as numerical feature vectors. \n",
    "\n",
    "The idea behind the bag-of-words model is quite simple and can be summarized as follows:\n",
    "1. We create a vocabulary of unique tokensâfor example, wordsâfrom the entire set of documents.\n",
    "2. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6Vf3cSwjNaV"
   },
   "source": [
    "### Vocabulary\n",
    "This function will create a vocabulary for each review and use it to get unigram features from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41247\n",
      "['\\x05', '\\x12', '\\x13', '\\x14', '\\x16', '.  .', '. .', '..', '...', \"a's\", 'a-bomb', 'a-changin', 'a-flailing', \"a-ha's\", 'a-hole', 'a-list', 'a-plenty', 'a-trois', 'a-week', 'aa-meetings', 'aaa', 'aaah', 'aaahhhs', 'aahs', 'aaliyah', \"aaliyah's\", 'aalyah', \"aalyah's\", 'aamir', 'aan', 'aardman', 'aarky', 'aaron', 'aast', 'aatish', \"aaya's\", 'ab', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abating', 'abba', 'abbe', 'abberation', 'abberline', 'abbot', 'abbott', 'abbotts', 'abbreviated', 'abby', \"abby's\", \"abc's\", 'abdomen', 'abducted', 'abduction', 'abdul-jabbar', 'abe', 'abel', 'abeled', 'aberdeen', 'aberration', 'abetting', 'abeyance', 'abhorrence', 'abhorrent', 'abider', 'abides', 'abigail', \"abigail's\", 'abiility', 'ability', 'abject', 'ablaze', 'able', 'ably', 'abney', 'abnormally', 'aboard', 'abode', 'abolish', 'abolitionist', 'abominable', 'abomination', 'aboo', 'aborginal', 'aboriginal', 'aboring', 'abort', 'aborted', 'abortion', 'abortionist', 'abortive', 'aboslutely', 'abound', 'abounding', 'abounds', \"about'the\", 'about-face', 'abouts', 'above-average', 'above-ground', 'above-mentioned', 'above-par', 'above-the-title', 'abraded', 'abraham', \"abraham's\", 'abrams', 'abrasive', 'abreast', 'abriel', 'abriela', \"abriela's\", 'abril', 'abrina', 'abroad', 'abrupt', 'abruptly', 'absence', 'absense', 'absent', 'absent-minded', 'absent-mindedness', 'absentee', 'absinthe', 'absoloute', 'absoltuely', 'absolut', 'absolute', 'absolutely', 'absolution', 'absolutist', 'absolved', 'absorb', 'absorbant', 'absorbed', 'absorbing', 'absorbs', 'absorption', 'abstract', 'abstraction', 'absurd', 'absurdist', 'absurdity', 'absurdly', 'abu', 'abundance', 'abundant', 'abundantly', 'abuse', 'abused', 'abuser', 'abusing', 'abusive', 'abute', \"abute's\", 'abuzz', 'aby', \"aby's\", 'abysmal', 'abysmally', 'abyss', 'abyssinian', 'ac', 'academe', 'academia', 'academic', 'academy', 'acaulay', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerator', 'accent', 'accentuate', 'accentuated', 'accentuating', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'acception', 'accepts', 'access', 'accessibility', 'accessible', 'accessorize', 'accessory', 'accident', \"accident's\", 'accidental', 'accidentally', 'accidently', 'acclaim', 'acclaimed', 'acclimatize', 'accolade', 'accommodate', 'accommodates', 'accommodating', 'accommodation', 'accomodates', 'accompanied', 'accompanies', 'accompaniment', 'accompany', 'accompanying', 'accomplice', 'accomplish', 'accomplished', 'accomplished-but-stiff', 'accomplishes', 'accomplishing', 'accomplishment', 'accord', 'accordance', 'according', 'accordingly', 'accordion', 'accosted', 'accosts', 'account', 'accountability', 'accountable', 'accountant', 'accounted', 'accumulated', 'accumulation', 'accuracy', 'accurate', 'accurately', 'accusation', 'accuse', 'accused', 'accuser', 'accuses', 'accusing', 'accustomed', 'acdonald', 'ace', 'ace-in-the-hole', 'aced', 'acerbic', 'acerbity', 'ach', 'acheivement', 'acheives', 'achel', 'acher', 'achievable', 'achieve', 'achieved', 'achieveing', 'achievement', 'achieves', 'achieving', 'achilles', 'achin', 'achine', 'achingly', 'achoo', 'acial', 'acid', 'acid-eaten', 'acidic', 'acing', 'acino', \"acino's\", 'acism', 'aciton', 'ack', 'acked', 'ackers', 'ackie', \"ackie's\", 'acking', 'ackland', 'ackman', 'acknowledge', 'acknowledgeable', 'acknowledged', 'acknowledges', 'acknowledging', 'acknowledgment', 'ackson', \"ackson's\", 'acme', 'acne', 'acnee', \"acnee's\", 'acob', \"acob's\", 'acon', 'aconic', 'acquaintance', \"acquaintances-i'd\", 'acquainted', 'acquaints', 'acqueline', 'acquire', 'acquired', 'acquires', 'acquisition', 'acquit', 'acquits', 'acquittal', 'acquitted', 'acre', 'acri', 'acrimonious', 'acrobat', 'acrobatic', 'acrobatics', 'acronym', 'across', 'across-the-board', 'acrosse', \"acrosse's\", 'acrylic', 'act', \"act's\", 'acted', 'actics', 'acting', 'acting-related', 'action', 'action-adventure', 'action-comedies', 'action-comedy', 'action-craving', 'action-crime', 'action-film', 'action-hero', 'action-horror', 'action-music-video', 'action-packed', 'action-thriller', 'action-verb', 'actioner', 'actioners', 'actionfest', 'actionless', 'activated', 'active', 'actively', 'activist', 'activites', 'activity', 'actor', \"actor's\", 'actor-turned-director', 'actor-wise', 'actress', \"actress's\", 'actress-turned-comedy', 'actress-type', 'actual', 'actualisation', 'actuality', 'actualizing', 'actually', 'actualy', 'acuity', 'acumen', 'acupuncture', 'acute', 'acutely', 'aczynski', 'ad', 'ad-lib', 'ad-libbed', 'adafarasin', 'adage', 'adagio', 'adair', 'adam', \"adam's\", 'adamantly', 'adandon', 'adapt', 'adaptable', 'adaptation', 'adaptation--in', 'adapted', 'adapter', 'adapting', 'adaption', 'add', 'add-in', \"addam's\", 'addams', 'added', 'adden', 'addict', 'addicted', 'addiction', 'addictive', 'adding', 'addington', 'addition', 'additional', 'additive', 'addled', 'address', 'addressed', 'addressing', 'addy', 'addyshack', 'ade', 'adefarasin', 'adelaide', 'adele', 'adeleine', 'adept', 'adeptly', 'adequate', 'adequately', 'adget', \"adget's\", 'adhere', 'adherence', 'adheres', 'adhesive', 'adian', 'adies', 'ading', 'adios', 'adjacent', 'adjective', 'adjoining', 'adjuster', 'adjusting', 'adjustment', 'adjusts', 'adlai', 'adlib', 'adly', 'administered', 'administration', 'administrative', 'admirable', 'admirably', 'admiral', 'admiration', 'admire', 'admired', 'admirer', 'admires', 'admiring', 'admission', 'admit', 'admiting', 'admits', 'admittance', 'admitted', 'admittedly', 'admitting', 'admittingly', 'admonition', 'ado', 'adolescence', 'adolescent', 'adolescent-minded', 'adolf', 'adolph', 'adonna', \"adonna's\", 'adopt', 'adopted', 'adopter', 'adopting', 'adoption', 'adoptive', 'adopts', 'adorable', 'adorableness', 'adorably', 'adoration', 'adore', 'adored', 'adores', 'adorned', 'adorning', 'adornment', 'adrenalin', 'adrenaline', 'adrian', 'adriana', 'adrianne', 'adrien', 'adrift', 'adroit', 'adroitly', 'adulation', 'adult']\n"
     ]
    }
   ],
   "source": [
    "#Required for Bag of words (unigram features) creation\n",
    "vocabulary = list(set(flat_list))\n",
    "vocabulary.sort() #sorting the list\n",
    "print(len(vocabulary))\n",
    "print(vocabulary[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrhj9ES3XrLt"
   },
   "source": [
    "### Unigram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 284606,
     "status": "ok",
     "timestamp": 1523491275626,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "dDpJCZSZsGb4",
    "outputId": "15c40085-a570-48e7-cb18-175713489d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 314.4381802082062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_unigram_features = get_unigram_features(mtrain,vocabulary) # vocabulary extracted in the beginning\n",
    "test_unigram_features     = get_unigram_features(mtest,vocabulary)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lf1yJbzMsyGf"
   },
   "source": [
    "### Sentiwordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88776,
     "status": "ok",
     "timestamp": 1523491412798,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "jcYBYcA9tEnw",
    "outputId": "24400ff8-54f1-43e8-969b-1c6b37e94d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 87.85296511650085 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "training_swn_features = get_senti_wordnet_features(mtrain)\n",
    "test_swn_features     = get_senti_wordnet_features(mtest)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2534,
     "status": "ok",
     "timestamp": 1523491535630,
     "user": {
      "displayName": "Sai Kiran",
      "photoUrl": "//lh6.googleusercontent.com/-g0SH7l3gv1U/AAAAAAAAAAI/AAAAAAAABNA/rDbBuTKANJc/s50-c-k-no/photo.jpg",
      "userId": "103874358019321492342"
     },
     "user_tz": -330
    },
    "id": "FfsBexgptcYg",
    "outputId": "dc4d1775-c7f2-4fdc-e856-7bd43809b139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.0876069068908691 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "mtraining_features = merge_features( training_unigram_features, training_swn_features)\n",
    "mtest_features     = merge_features( test_unigram_features,     test_swn_features)\n",
    "\n",
    "## These are the final set of features on which training and testing can ber performed\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of NB classifier is\n",
      "Training data\t0.99\n",
      "Test data\t0.836\n",
      "--- 19.87365460395813 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = MultinomialNB(alpha=0.1).fit(mtraining_features, mtraining_labels) #training process\n",
    "\n",
    "print(\"Precision of NB classifier is\")\n",
    "\n",
    "pred = nb_classifier.predict(mtraining_features)\n",
    "precision = calculate_precision(pred, mtraining_labels)\n",
    "\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "pred = nb_classifier.predict(mtest_features)\n",
    "\n",
    "movie_nb = calculate_precision(pred, mtest_gold_labels)\n",
    "print(\"Test data\\t\" + str(movie_nb))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Decision Tree classifier is\n",
      "Training data\t0.6226666666666667\n",
      "Test data\t0.588\n",
      "--- 13.158593654632568 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=5, max_features='sqrt', random_state=42).fit(mtraining_features, mtraining_labels)\n",
    "\n",
    "print(\"Precision of Decision Tree classifier is\")\n",
    "\n",
    "pred = clf.predict(mtraining_features)\n",
    "precision = calculate_precision(pred, mtraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(mtest_features)\n",
    "movie_dt = calculate_precision(pred, mtest_gold_labels)\n",
    "print(\"Test data\\t\" + str(movie_dt))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Logistic Regression is\n",
      "Training data\t1.0\n",
      "Test data\t0.83\n",
      "--- 15.364763736724854 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=42).fit(mtraining_features, mtraining_labels)\n",
    "print(\"Precision of Logistic Regression is\")\n",
    "\n",
    "pred = clf.predict(mtraining_features)\n",
    "precision = calculate_precision(pred, mtraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(mtest_features)\n",
    "movie_lr = calculate_precision(pred, mtest_gold_labels)\n",
    "print(\"Test data\\t\" + str(movie_lr))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of SVM classifier is\n",
      "Training data\t0.5026666666666667\n",
      "Test data\t0.492\n",
      "--- 347.9355294704437 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=0.01, kernel='sigmoid').fit(mtraining_features, mtraining_labels)\n",
    "print(\"Precision of SVM classifier is\")\n",
    "\n",
    "pred = clf.predict(mtraining_features)\n",
    "precision = calculate_precision(pred, mtraining_labels)\n",
    "print(\"Training data\\t\" + str(precision))\n",
    "\n",
    "pred = clf.predict(mtest_features)\n",
    "movie_svm = calculate_precision(pred, mtest_gold_labels)\n",
    "print(\"Test data\\t\" + str(movie_svm))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Movie Review</th>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.5880</td>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twitter Dataset</th>\n",
       "      <td>0.7376</td>\n",
       "      <td>0.494667</td>\n",
       "      <td>0.5852</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Naive Bayes       SVM  Decision Tree  Logistic Regression\n",
       "Score                                                                     \n",
       "Movie Review          0.8360  0.492000         0.5880                0.830\n",
       "Twitter Dataset       0.7376  0.494667         0.5852                0.758"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets make score card for each model in tabular form using pandas\n",
    "import pandas as pd\n",
    "\n",
    "list_of_lists = []\n",
    "list_of_lists.append(['Movie Review', movie_nb, movie_svm, movie_dt, movie_lr])\n",
    "list_of_lists.append(['Twitter Dataset', twitter_nb, twitter_svm, twitter_dt, twitter_lr])\n",
    "\n",
    "\n",
    "Score = pd.DataFrame(list_of_lists, columns=['Score','Naive Bayes','SVM', 'Decision Tree', 'Logistic Regression'])\n",
    "\n",
    "Score = Score.set_index('Score')\n",
    "Score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Assignment2_NLP_Saikiran_Sentiment_Analysis_11_Apr_2018.ipynb",
   "provenance": [
    {
     "file_id": "1hD10pdFMqyLXPj7sQBLQhjjV0COER-4D",
     "timestamp": 1522558957966
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
